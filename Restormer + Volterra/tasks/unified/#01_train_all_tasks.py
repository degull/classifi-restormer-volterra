""" import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, Subset
from torchvision import transforms
from tqdm import tqdm
from torch.amp import autocast, GradScaler
from PIL import Image
import pandas as pd
import numpy as np
import random
from skimage.metrics import peak_signal_noise_ratio as compute_psnr
from skimage.metrics import structural_similarity as compute_ssim

# âœ… ëª¨ë¸ import ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from models.restormer_volterra import RestormerVolterra


# ------------------ Dataset ì •ì˜ ------------------
class PairedFolderDataset(torch.utils.data.Dataset):
    def __init__(self, input_dir, target_dir, transform):
        valid_ext = (".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff")
        self.input_paths = sorted([
            os.path.join(input_dir, f)
            for f in os.listdir(input_dir)
            if f.lower().endswith(valid_ext)
        ])
        self.target_paths = []
        for inp_path in self.input_paths:
            fname = os.path.basename(inp_path)
            found = False
            for ext in valid_ext:
                gt_path = os.path.join(target_dir, os.path.splitext(fname)[0] + ext)
                if os.path.exists(gt_path):
                    self.target_paths.append(gt_path)
                    found = True
                    break
            if not found:
                print(f"[âš ï¸] GT not found for {fname}")
                self.input_paths.remove(inp_path)

        assert len(self.input_paths) == len(self.target_paths), "Input/target mismatch"
        self.transform = transform

    def __len__(self):
        return len(self.input_paths)

    def __getitem__(self, idx):
        inp = Image.open(self.input_paths[idx]).convert("RGB")
        tgt = Image.open(self.target_paths[idx]).convert("RGB")
        return self.transform(inp), self.transform(tgt)


class PairedCSVDataset(torch.utils.data.Dataset):
    def __init__(self, csv_path, transform):
        df = pd.read_csv(csv_path)
        self.paths = df.values.tolist()
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        inp_path, tgt_path = self.paths[idx]
        inp = Image.open(inp_path).convert("RGB")
        tgt = Image.open(tgt_path).convert("RGB")
        return self.transform(inp), self.transform(tgt)


# ------------------ ì„¤ì • ------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SAVE_DIR = r"E:/restormer+volterra/checkpoints/#01_all_tasks_balanced_160"
os.makedirs(SAVE_DIR, exist_ok=True)

START_EPOCH = 1     # âœ… ì²˜ìŒë¶€í„° í•™ìŠµ
EPOCHS = 100
BATCH_SIZE = 2
LR = 2e-4
MAX_SAMPLES = 160  # ê° ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜


# ------------------ Transform ------------------
resize_schedule = {0: 128, 30: 192, 60: 256}
def get_transform(epoch):
    size = max(v for k, v in resize_schedule.items() if epoch >= k)
    return transforms.Compose([
        transforms.Resize((size, size), interpolation=Image.BICUBIC),
        transforms.ToTensor()
    ])


# ------------------ Dataset ìƒ˜í”Œë§ ------------------
def subset_dataset(dataset, max_samples):
    total = len(dataset)
    if total <= max_samples:
        return dataset
    indices = random.sample(range(total), max_samples)
    return Subset(dataset, indices)


# ------------------ Dataset ë¡œë”© ------------------
def get_all_datasets(transform):
    print("\nğŸ“¦ Preparing balanced datasets...")

    datasets = []

    rain100h = subset_dataset(PairedFolderDataset(
        r"E:/restormer+volterra/data/rain100H/train/rain",
        r"E:/restormer+volterra/data/rain100H/train/norain",
        transform
    ), MAX_SAMPLES)
    print(f"Rain100H      : {len(rain100h)} samples")
    datasets.append(rain100h)

    rain100l = subset_dataset(PairedFolderDataset(
        r"E:/restormer+volterra/data/rain100L/train/rain",
        r"E:/restormer+volterra/data/rain100L/train/norain",
        transform
    ), MAX_SAMPLES)
    print(f"Rain100L      : {len(rain100l)} samples")
    datasets.append(rain100l)

    gopro = subset_dataset(PairedCSVDataset(
        r"E:/restormer+volterra/data/GOPRO_Large/gopro_train_pairs.csv",
        transform
    ), MAX_SAMPLES)
    print(f"GoPro         : {len(gopro)} samples")
    datasets.append(gopro)

    sidd = subset_dataset(PairedCSVDataset(
        r"E:/restormer+volterra/data/SIDD/sidd_pairs.csv",
        transform
    ), MAX_SAMPLES)
    print(f"SIDD          : {len(sidd)} samples")
    datasets.append(sidd)

    csd = subset_dataset(PairedFolderDataset(
        r"E:/restormer+volterra/data/CSD/Train/Snow",
        r"E:/restormer+volterra/data/CSD/Train/Gt",
        transform
    ), MAX_SAMPLES)
    print(f"CSD           : {len(csd)} samples")
    datasets.append(csd)

    total = sum(len(d) for d in datasets)
    print(f"ğŸ“Š Total training samples (balanced): {total} images\n")

    return ConcatDataset(datasets)


# ------------------ í•™ìŠµ ë£¨í”„ ------------------
def train_all_tasks():
    model = RestormerVolterra().to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    scaler = GradScaler()
    criterion = nn.L1Loss()

    print("\nğŸš€ Training from scratch...\n")

    for epoch in range(START_EPOCH, EPOCHS + 1):
        transform = get_transform(epoch)
        dataset = get_all_datasets(transform)
        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,
                                  num_workers=4, pin_memory=True)

        model.train()
        total_loss = total_psnr = total_ssim = count = 0

        loop = tqdm(train_loader, desc=f"[Epoch {epoch}/{EPOCHS}]")
        for inputs, targets in loop:
            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)

            with autocast(device_type='cuda'):
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

            # Metric ê³„ì‚°
            outputs = outputs.detach().clamp(0, 1).cpu().numpy()
            targets = targets.cpu().numpy()
            for out, gt in zip(outputs, targets):
                out = np.transpose(out, (1, 2, 0))
                gt = np.transpose(gt, (1, 2, 0))
                total_psnr += compute_psnr(gt, out, data_range=1.0)
                total_ssim += compute_ssim(gt, out, data_range=1.0, channel_axis=-1)
                count += 1

            loop.set_postfix(loss=loss.item())

        avg_psnr = total_psnr / count
        avg_ssim = total_ssim / count
        print(f"ğŸ“£ Epoch {epoch:03d}: PSNR={avg_psnr:.2f}, SSIM={avg_ssim:.4f}")

        ckpt_name = f"epoch_{epoch}_ssim{avg_ssim:.4f}_psnr{avg_psnr:.2f}.pth"
        torch.save(model.state_dict(), os.path.join(SAVE_DIR, ckpt_name))


# ------------------ ì‹¤í–‰ ------------------
if __name__ == "__main__":
    train_all_tasks() """


# ì´ì–´ì„œ í•™ìŠµ
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, Subset
from torchvision import transforms
from tqdm import tqdm
from torch.amp import autocast, GradScaler
from PIL import Image
import pandas as pd
import numpy as np
import random
from skimage.metrics import peak_signal_noise_ratio as compute_psnr
from skimage.metrics import structural_similarity as compute_ssim

# âœ… ëª¨ë¸ import ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from models.restormer_volterra import RestormerVolterra


# ------------------ Dataset ì •ì˜ ------------------
class PairedFolderDataset(torch.utils.data.Dataset):
    def __init__(self, input_dir, target_dir, transform):
        valid_ext = (".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff")
        self.input_paths = sorted([
            os.path.join(input_dir, f)
            for f in os.listdir(input_dir)
            if f.lower().endswith(valid_ext)
        ])
        self.target_paths = []
        for inp_path in self.input_paths:
            fname = os.path.basename(inp_path)
            found = False
            for ext in valid_ext:
                gt_path = os.path.join(target_dir, os.path.splitext(fname)[0] + ext)
                if os.path.exists(gt_path):
                    self.target_paths.append(gt_path)
                    found = True
                    break
            if not found:
                print(f"[âš ï¸] GT not found for {fname}")
                self.input_paths.remove(inp_path)

        assert len(self.input_paths) == len(self.target_paths), "Input/target mismatch"
        self.transform = transform

    def __len__(self):
        return len(self.input_paths)

    def __getitem__(self, idx):
        inp = Image.open(self.input_paths[idx]).convert("RGB")
        tgt = Image.open(self.target_paths[idx]).convert("RGB")
        return self.transform(inp), self.transform(tgt)


class PairedCSVDataset(torch.utils.data.Dataset):
    def __init__(self, csv_path, transform):
        df = pd.read_csv(csv_path)
        self.paths = df.values.tolist()
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        inp_path, tgt_path = self.paths[idx]
        inp = Image.open(inp_path).convert("RGB")
        tgt = Image.open(tgt_path).convert("RGB")
        return self.transform(inp), self.transform(tgt)


# ------------------ ì„¤ì • ------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SAVE_DIR = r"E:/restormer+volterra/checkpoints/#01_all_tasks_balanced_160"
os.makedirs(SAVE_DIR, exist_ok=True)

# âœ… ì´ì–´ì„œ í•™ìŠµ
RESUME_PATH = os.path.join(
    SAVE_DIR,
    "epoch_100_ssim0.9177_psnr32.58.pth"
)

START_EPOCH = 101   # ì´ì–´ì„œ ì‹œì‘í•  epoch
EPOCHS = 200        # ìµœì¢… epoch
BATCH_SIZE = 2
LR = 2e-4
MAX_SAMPLES = 160   # ê° ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜


# ------------------ Transform ------------------
resize_schedule = {0: 128, 30: 192, 60: 256}
def get_transform(epoch):
    size = max(v for k, v in resize_schedule.items() if epoch >= k)
    return transforms.Compose([
        transforms.Resize((size, size), interpolation=Image.BICUBIC),
        transforms.ToTensor()
    ])


# ------------------ Dataset ìƒ˜í”Œë§ ------------------
def subset_dataset(dataset, max_samples):
    total = len(dataset)
    if total <= max_samples:
        return dataset
    indices = random.sample(range(total), max_samples)
    return Subset(dataset, indices)


# ------------------ Dataset ë¡œë”© ------------------
def get_all_datasets(transform):
    print("\nğŸ“¦ Preparing balanced datasets...")

    datasets = []

    rain100h = subset_dataset(PairedFolderDataset(
        r"E:/restormer+volterra/data/rain100H/train/rain",
        r"E:/restormer+volterra/data/rain100H/train/norain",
        transform
    ), MAX_SAMPLES)
    print(f"Rain100H      : {len(rain100h)} samples")
    datasets.append(rain100h)

    rain100l = subset_dataset(PairedFolderDataset(
        r"E:/restormer+volterra/data/rain100L/train/rain",
        r"E:/restormer+volterra/data/rain100L/train/norain",
        transform
    ), MAX_SAMPLES)
    print(f"Rain100L      : {len(rain100l)} samples")
    datasets.append(rain100l)

    gopro = subset_dataset(PairedCSVDataset(
        r"E:/restormer+volterra/data/GOPRO_Large/gopro_train_pairs.csv",
        transform
    ), MAX_SAMPLES)
    print(f"GoPro         : {len(gopro)} samples")
    datasets.append(gopro)

    sidd = subset_dataset(PairedCSVDataset(
        r"E:/restormer+volterra/data/SIDD/sidd_pairs.csv",
        transform
    ), MAX_SAMPLES)
    print(f"SIDD          : {len(sidd)} samples")
    datasets.append(sidd)

    csd = subset_dataset(PairedFolderDataset(
        r"E:/restormer+volterra/data/CSD/Train/Snow",
        r"E:/restormer+volterra/data/CSD/Train/Gt",
        transform
    ), MAX_SAMPLES)
    print(f"CSD           : {len(csd)} samples")
    datasets.append(csd)

    total = sum(len(d) for d in datasets)
    print(f"ğŸ“Š Total training samples (balanced): {total} images\n")

    return ConcatDataset(datasets)


# ------------------ í•™ìŠµ ë£¨í”„ ------------------
def train_all_tasks():
    model = RestormerVolterra().to(DEVICE)

    # âœ… ì´ì „ ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(RESUME_PATH):
        model.load_state_dict(torch.load(RESUME_PATH, map_location=DEVICE))
        print(f"âœ… Resumed from: {RESUME_PATH}")
    else:
        print(f"[âš ï¸] Resume checkpoint not found: {RESUME_PATH}")
        return

    optimizer = optim.AdamW(model.parameters(), lr=LR)
    scaler = GradScaler()
    criterion = nn.L1Loss()

    print("\nğŸš€ Resuming Training...\n")

    for epoch in range(START_EPOCH, EPOCHS + 1):
        transform = get_transform(epoch)
        dataset = get_all_datasets(transform)
        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,
                                  num_workers=4, pin_memory=True)

        model.train()
        total_loss = total_psnr = total_ssim = count = 0

        loop = tqdm(train_loader, desc=f"[Epoch {epoch}/{EPOCHS}]")
        for inputs, targets in loop:
            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)

            with autocast(device_type='cuda'):
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

            # Metric ê³„ì‚°
            outputs = outputs.detach().clamp(0, 1).cpu().numpy()
            targets = targets.cpu().numpy()
            for out, gt in zip(outputs, targets):
                out = np.transpose(out, (1, 2, 0))
                gt = np.transpose(gt, (1, 2, 0))
                total_psnr += compute_psnr(gt, out, data_range=1.0)
                total_ssim += compute_ssim(gt, out, data_range=1.0, channel_axis=-1)
                count += 1

            loop.set_postfix(loss=loss.item())

        avg_psnr = total_psnr / count
        avg_ssim = total_ssim / count
        print(f"ğŸ“£ Epoch {epoch:03d}: PSNR={avg_psnr:.2f}, SSIM={avg_ssim:.4f}")

        ckpt_name = f"epoch_{epoch}_ssim{avg_ssim:.4f}_psnr{avg_psnr:.2f}.pth"
        torch.save(model.state_dict(), os.path.join(SAVE_DIR, ckpt_name))


# ------------------ ì‹¤í–‰ ------------------
if __name__ == "__main__":
    train_all_tasks()
